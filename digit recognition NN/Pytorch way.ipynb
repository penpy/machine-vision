{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ' + str(device))\n",
    "\n",
    "size_input = 784\n",
    "size_hidden = 200\n",
    "n_classes = 100\n",
    "\n",
    "n_epochs = 2\n",
    "batch_size = 100\n",
    "n_iter = n_epochs // batch_size\n",
    "learn_r = 0.001\n",
    "\n",
    "path = ''\n",
    "train_data = torchvision.datasets.MNIST(root=path, train=True, transform=torchvision.transforms.ToTensor())\n",
    "test_data = torchvision.datasets.MNIST(root=path, train=False, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=10000, shuffle=False)\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.lin_1 = nn.Linear(size_input, size_hidden)\n",
    "        self.lin_2 = nn.Linear(size_hidden, n_classes)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        out_1 = torch.relu(self.lin_1(inp))\n",
    "        out_2 = self.lin_2(out_1)\n",
    "        return out_2\n",
    "\n",
    "model = NeuralNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(2.3082, grad_fn=<NllLossBackward>)\n",
      "0 tensor(0.4485, grad_fn=<NllLossBackward>)\n",
      "0 tensor(0.4349, grad_fn=<NllLossBackward>)\n",
      "0 tensor(0.2091, grad_fn=<NllLossBackward>)\n",
      "0 tensor(0.2543, grad_fn=<NllLossBackward>)\n",
      "0 tensor(0.2631, grad_fn=<NllLossBackward>)\n",
      "1 tensor(0.2615, grad_fn=<NllLossBackward>)\n",
      "1 tensor(0.1242, grad_fn=<NllLossBackward>)\n",
      "1 tensor(0.1959, grad_fn=<NllLossBackward>)\n",
      "1 tensor(0.1822, grad_fn=<NllLossBackward>)\n",
      "1 tensor(0.2040, grad_fn=<NllLossBackward>)\n",
      "1 tensor(0.1528, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learn_r)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        images, labels = batch\n",
    "        flat_images = torch.reshape(images, (-1, size_input)).to(device)\n",
    "        y_pred = model(flat_images).to(device)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i%100==0:\n",
    "            print(epoch, loss)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9526\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for test_data in test_loader:\n",
    "        images, labels = test_data\n",
    "        flat_images = torch.reshape(images, (-1, size_input))\n",
    "        y_pred = model(flat_images)\n",
    "        max_inidices = torch.max(y_pred, dim=1)[1]\n",
    "        n_correct = (labels == max_inidices).sum()\n",
    "\n",
    "        print(n_correct.item()/10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction : 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_iter = iter(test_loader)\n",
    "test_images, test_labels = test_iter.next()\n",
    "\n",
    "def test():\n",
    "    i = np.random.randint(10000)\n",
    "    image = test_images[i,0]\n",
    "    label = test_labels[i]\n",
    "    plt.imshow(image, cmap='Greys')\n",
    "    plt.title('label = ' + str(label.item()))\n",
    "    with torch.no_grad():\n",
    "        flat_image = torch.reshape(image, (1, 784))\n",
    "        y_pred = model(flat_image)\n",
    "        max_index = torch.argmax(y_pred).item()\n",
    "        print('prediction :', max_index)\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
